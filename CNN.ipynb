{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9246394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import import_ipynb\n",
    "from ANN import Layer_Dense, Activation_Softmax, Loss_CategoricalCrossentropy, Activation_Softmax_Loss_CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5707665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Conv:\n",
    "    def __init__(self, n_filters, kernel_size):\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Filters are 3D: (num_filters, kernel_height, kernel_width)\n",
    "        # We divide by kernel_size**2 to keep weights small (He initialization-ish)\n",
    "        self.filters = np.random.randn(n_filters, kernel_size, kernel_size) / (kernel_size**2)\n",
    "\n",
    "        # One bias for each filter\n",
    "        self.biases = np.zeros(n_filters)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input # Save for backprop\n",
    "        h, w = input.shape\n",
    "\n",
    "        # Calculate output dimensions: (Input_size - Filter_size) + 1\n",
    "        # (Assuming stride=1 and no padding for this simple version)\n",
    "        output_h = h - self.kernel_size + 1\n",
    "        output_w = w - self.kernel_size + 1\n",
    "\n",
    "        # Initialize output: (number of filters, new_height, new_width)\n",
    "        self.output = np.zeros((self.n_filters, output_h, output_w))\n",
    "\n",
    "        # The \"Sliding Window\" Loops\n",
    "        for i in range(output_h):\n",
    "            for j in range(output_w):\n",
    "                # 1. Extract the current patch (region of interest)\n",
    "                im_region = input[i:(i + self.kernel_size), j:(j + self.kernel_size)]\n",
    "\n",
    "                # 2. Element-wise multiply the patch with ALL filters and sum\n",
    "                # np.sum over the axes of the kernel height/width\n",
    "                self.output[:, i, j] = np.sum(im_region * self.filters, axis=(1, 2)) + self.biases\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, d_l_d_out, learning_rate):\n",
    "        \"\"\"\n",
    "        d_l_d_out: The gradient of the loss with respect to the output of this layer.\n",
    "        \"\"\"\n",
    "        d_l_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        h, w = self.last_input.shape\n",
    "\n",
    "        for i in range(h - self.kernel_size + 1):\n",
    "            for j in range(w - self.kernel_size + 1):\n",
    "                # For every region the filter touched during forward pass...\n",
    "                im_region = self.last_input[i:(i + self.kernel_size), j:(j + self.kernel_size)]\n",
    "\n",
    "                # For each filter, the gradient is the region multiplied by\n",
    "                # the gradient of the output at that specific (i, j) position\n",
    "                for f in range(self.n_filters):\n",
    "                    d_l_d_filters[f] += d_l_d_out[f, i, j] * im_region\n",
    "\n",
    "        # Update weights (SGD)\n",
    "        self.filters -= learning_rate * d_l_d_filters\n",
    "        self.biases -= learning_rate * np.sum(d_l_d_out, axis=(1, 2))\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d71c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2:\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        num_filters, h, w = input.shape\n",
    "        # Output is half the size (stride=2)\n",
    "        self.output = np.zeros((num_filters, h // 2, w // 2))\n",
    "\n",
    "        for i in range(h // 2):\n",
    "            for j in range(w // 2):\n",
    "                im_region = input[:, (i*2):(i*2+2), (j*2):(j*2+2)]\n",
    "                self.output[:, i, j] = np.amax(im_region, axis=(1, 2))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_l_d_out):\n",
    "        # We only pass the gradient to the pixel that was the MAX\n",
    "        d_l_d_input = np.zeros(self.last_input.shape)\n",
    "        num_filters, h, w = self.last_input.shape\n",
    "\n",
    "        for i in range(h // 2):\n",
    "            for j in range(w // 2):\n",
    "                im_region = self.last_input[:, (i*2):(i*2+2), (j*2):(j*2+2)]\n",
    "                # Find the index of the max value in the region\n",
    "                for f in range(num_filters):\n",
    "                    # Get coordinates of the maximum value in this 2x2 patch\n",
    "                    idx = np.unravel_index(np.argmax(im_region[f]), (2, 2))\n",
    "                    d_l_d_input[f, i*2 + idx[0], j*2 + idx[1]] = d_l_d_out[f, i, j]\n",
    "        return d_l_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dcd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Flatten:\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        return input.flatten().reshape(1, -1)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Reshape the 1D gradient back to the 3D shape of the input\n",
    "        return dvalues.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "277fde1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 2.3213\n",
      "Backward pass complete. Filters updated!\n"
     ]
    }
   ],
   "source": [
    "conv = Layer_Conv(8, 3)                  # 8 filters, 3x3 size\n",
    "pool = MaxPool2()                        # 2x2 Max Pooling\n",
    "flatten = Layer_Flatten()\n",
    "# After conv(26x26) and pool(13x13), we have 8 * 13 * 13 = 1352 inputs\n",
    "dense = Layer_Dense(1352, 10)            # 10 output classes (digits 0-9)\n",
    "loss_softmax = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# FORWARD PASS\n",
    "image = np.random.randn(28, 28)          # A \"fake\" 28x28 grayscale image\n",
    "label = 7                                # The \"true\" digit is 7\n",
    "\n",
    "# 1. Convolution\n",
    "out = conv.forward(image)                # Shape: (8, 26, 26)\n",
    "# 2. ReLU\n",
    "out = np.maximum(0, out)                 # (Simple ReLU)\n",
    "# 3. Pooling\n",
    "out = pool.forward(out)                  # Shape: (8, 13, 13)\n",
    "# 4. Flatten\n",
    "out = flatten.forward(out)               # Shape: (1, 1352)\n",
    "# 5. Dense + Softmax/Loss\n",
    "loss = loss_softmax.forward(dense.forward(out) or dense.output, np.array([label]))\n",
    "\n",
    "print(f\"Initial Loss: {loss:.4f}\")\n",
    "\n",
    "# BACKWARD PASS \n",
    "# 1. Loss -> Dense\n",
    "loss_softmax.backward(loss_softmax.output, np.array([label]))\n",
    "dense.backward(loss_softmax.dinputs)\n",
    "\n",
    "# 2. Dense -> Flatten\n",
    "d_flatten = flatten.backward(dense.dinputs)\n",
    "\n",
    "# 3. Flatten -> Pooling\n",
    "d_pool = pool.backward(d_flatten)\n",
    "\n",
    "# 4. Pooling -> Conv\n",
    "# We modify our Conv class slightly to accept d_pool and update its filters\n",
    "conv.backward(d_pool, learning_rate=0.01)\n",
    "\n",
    "print(\"Backward pass complete. Filters updated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
